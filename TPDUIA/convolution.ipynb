{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pourquoi la convolution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un des pré-requis à l'entraînement de réseaux de neurones dédiés au traitement de l'image est aujourd'hui la **convolution**.\n",
    "\n",
    "Les réseaux de neurones convolutifs, les _CNN_, occupent aujourd'hui en santé une place majeure : classification, segmentation, survie, marqueurs pronostics.\n",
    "\n",
    "Pourquoi utilise-t-on aujourd'hui _presque_ exclusivement ce type d'opération mathématique pour analyser nos images ?\n",
    "\n",
    "Réponse courte : les _CNN_ peuvent extraire automatiquement des informations morphologiques qui échappent à un oeil expert. Ces réseaux, par leur complexité extrême, permettent de capturer la complexité inhérente aux images qu'ils analysent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brève histoire de l'apprentissage profond (et de la convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Première modélisation mathématique et informatique d’un neurone biologique : W. McCulloch et W. Pitts (1943).**\n",
    "\n",
    "<img src=\"https://cs231n.github.io/assets/nn1/neuron.png\" width=\"400\">\n",
    "\n",
    "<img src=\"https://cs231n.github.io/assets/nn1/neuron_model.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier modèle mathématique et informatique du neurone biologique est proposé par Warren McCulloch et Walter Pitts en 1943. En s'appuyant sur les propriétés des neurones biologiques connues à cette époque, issues d'observations neurophysiologiques et anatomiques, McCulloch et Pitts proposent un modèle simple de neurone formel. Il s'agit d'un neurone binaire, c'est-à-dire dont la sortie vaut 0 ou 1. Pour calculer cette sortie, le neurone effectue une somme pondérée de ses entrées. Ces entrées ($x_0, x_1, ..., x_N$) sont perçues au niveau des synapses (région de contact), et en tant que sorties d'autres neurones formels, valent aussi 0 ou 1. Ces stimulations, avant d'être transmis au coeur de la cellule, sont pondérées ($w_0, w_1, ..., w_N$) de telle sorte qu'aux différentes informations transmises (_e.g._ influx électriques), une priorisation a lieu, et _a fortiori_ un blocage de certaines informations $w_i=0$. Les signaux plus ou moins atténués (inhibés) ou renforcés (excités) ($w_i x_i$) sont transmis par les dendrites vers le coeur de la cellule. Cette somme pondérée de signaux active ou non le neurone qui applique une fonction d'actiation à un seuil : si la somme pondérée dépasse une certaine valeur, la sortie du neurone est 1, sinon elle vaut 0 (cf les sections suivantes). ([1])\n",
    "\n",
    "**Malgré la simplicité de cette modélisation, ou peut-être grâce à elle, le neurone formel dit de McCulloch et Pitts reste aujourd’hui un élément de base des réseaux de neurones artificiels. De nombreuses variantes ont été proposées, plus ou moins biologiquement plausibles, mais s'appuyant généralement sur les concepts inventés par les deux auteurs. On sait néanmoins aujourd’hui que ce modèle n’est qu’une approximation des fonctions remplies par le neurone réel et, qu’en aucune façon, il ne peut servir pour une compréhension profonde du système nerveux.** ([1])\n",
    "\n",
    "Dans leur formulation d'origine, la fonction d'activation est la fonction de _Heaviside_, qui est une marche d'escalier binaire (elle passe de 0 à 1 en $x=0$).\n",
    "\n",
    "[1] https://fr.wikipedia.org/wiki/Neurone_formel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le perceptron de Rosenblatt (1957)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le perceptron est l'ancêtre du réseau de neurones, et en cela sa version la plus simple. Un perceptron dispose de $N$ entrées $(x_1,..., x_N)$ et une seule sortie $\\hat{y}$. Cette sortie est définie comme précédemment : $\\hat{y}=1$ si $\\sum_i w_i x_i > b$ sinon 0.\n",
    "\n",
    "Si on note $z = \\sum_i w_i x_i - b$, alors, $\\hat{y}=H(z)$ avec $H$ la fonction de Heaviside, qui vaut $0$ si $z < 0$, $1$ si $z \\geq 0$.\n",
    "\n",
    "<img src=\"https://ars.els-cdn.com/content/image/3-s2.0-B9780128124635000029-f02-02-9780128124635.gif\" width=\"200\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La rgèle d'apprentissage du perceptron ou loi de Widrow-Hoff (1949).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La règle de Widrow-Hoff est légèrement différente de la première règle d'apprentissage des réseaux de neurones artificiels de Hebb. \n",
    "Voici la règle de W-H :\n",
    "\n",
    "$$w_i^{(t+1)} \\leftarrow w_i^{(t)} + \\alpha (y-\\hat{y}^{(t)}) x_i$$\n",
    "\n",
    "avec $\\alpha$ le fameux taux d'apprentissage.\n",
    "\n",
    "Admettons que $y-\\hat{y}^{(t)} \\neq 0$, cela correspond à 2 cas de figures :\n",
    "- soit $y=1$ et $\\hat{y}^{(t)}=0$ (différence positive, cas 1)\n",
    "- soit $y=0$ et $\\hat{y}^{(t)}=1$ (différence négative, cas 2)\n",
    "\n",
    "Dans le premier cas, on s'aperçoit donc que le neurone formel, à l'itération $t$, a \"conclu\" qu'il ne devait transmettre d'information au vu des différents signaux d'entrée $(x_1, ..., x_N)$; alors qu'en réalité, il aurait dû ! Dans ce cas, la différence est bien positive. Pour y remédier, on va augmenter le poids associé à l'entrée binaire $x_i$ avec la règle ci-dessus. Si $x_i=0$, $w_i$ reste inchangé (c'est normal, il n'y a de toute façon pas d'information à transmettre). Mais si $x_i=1$, alors $w_i$ devient $w_i + \\alpha$. \n",
    "Dans le second cas, on procède à l'inverse : le poids donné à l'information $x_i$ étant trop élevée, on l'abaisse. \n",
    "\n",
    "Si $\\alpha$ est grand, alors les modifications des poids pour les $x_i=1$ vont être d'autant plus grande : on va donc très rapidement explorer l'espace, on apprend vite au risque cependant de ne pas tomber dans la zone de l'espace qui nous garantit les poids optimaux.\n",
    "A l'inverse, si $\\alpha$ est petit, nous sommes \"prudents\" et préférons exploiter l'espace de recherche, en explorant peu. L'avantage est, si l'on trouve une zone de l'espace d'optimisation favorable, de suffisamment l'exploiter pour arriver à une configuration presque optimale. En revanche, si, dès le départ, les poids sont modifiés selon une configuration sous-optimale, on risque de rester dans cette région de l'espace.\n",
    "\n",
    "**Cette règle est très semblable à l'algorithme de descente de gradient, qui est à l'oeuvre dans les processus d'entraînement des réseaux de neurones artificiels.**\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg\" width=\"400\">\n",
    "\n",
    "<img src=\"http://ludovicarnold.altervista.org/wp-content/uploads/2015/01/gradient-trajectory.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Théorème d'approximation universelle de Hornik (1991).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hornik montre en 1991 que toute fonction bornée et régulière de $\\mathbb{R}^d \\rightarrow \\mathbb{R}$ peut être approximée par un perceptron possédant un nombre **fini** de neurones, la même fonction d'activation $\\phi$, et une sortie linéaire, ceci pour n'importe quelle précision $\\epsilon$.\n",
    "\n",
    "Une fonction régulière est _grosso modo_ une fonction infiniment dérivable $\\mathcal{C}^{\\infty}$, comme ici une fonction s'exprimant en 2D.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/24/Bump2D_illustration.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Du percetron mono-couche à multi-couches (1957).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'inconvénient majeur des premiers réseaux de neurones est qu'il n'étaient pas en mesure de résoudre des problèmes non linéaires. En effet, historiquement, les fonctions d'activation utilisées dans le cadre de l'implémentation de ces algorithmes étaient tout au plus linéaires. Or, il est facile de montrer que si l'on empile les couches de neurones les unes après les autres, et que l'on utilise des activations linéaires (c'est à dire une fonction linéaire qui _map_ les entrées pondérées à la sortie), alors n'importe quel perceptron multi-couches de ce type peut être réduit à un perceptron à 2 couches : entrée et sortie, arbitrairement grand.\n",
    "\n",
    "L'introduction de fonctions d'activations non-linéaires comme la fonction sigmoide ou tangente hyperbolique, a changé la donne : désormais, il est possible d'activer non-linéairement un neurone; et donc d'introduire des relations non-linéaires entre les sorties et les entrées.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png\" width=\"400\">\n",
    "\n",
    "<img src=\"https://static.javatpoint.com/tutorial/tensorflow/images/single-layer-perceptron-in-tensorflow2.png\" width=\"400\">\n",
    "\n",
    "<img src=\"https://playandlearntocode.com/static/siteapp/assets/articles/mlp-1/mlp-1-nodes.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un perceptron multi-couche est une simple généralisation du perceptron original. Il est composé d'une couche d'entrée, des couches intermédiaires dites \"cachées\", et une couche de sortie. Chacune des neurones utilise des activations non-linéaires. \n",
    "\n",
    "Les travaux de Paul Werbos (1974) puis David Rumelhart (1986), inventeurs de la _backpropagation_, ont rendu possible l'optimisation des MLP (_Multi Layer Perceptron_). En effet, cette méthode permet d'utiliser n'importe quelle fonction d'activation non-linéaire, du moment que celle-ci est différentiable, comme la fonction sigmoid ou ReLU par exemple.\n",
    "\n",
    "La mise en place d'un MLP pour résoudre un problème de régression ou de classification, nécessite de déterminer de manière optimale les poids applicables à chacune des connexions inter-neuronales (sur le schéma, $w_{i,j,k}$ avec $i$ la couche cachée, $j$ le neurone d'entrée de la couche cachée $i$ et $k$ le neurone de sortie de la couche $i+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation.**\n",
    "_Schémas tirés du très bon blog : https://ranasinghiitkgp.medium.com/non-linear-binary-classification-using-neural-network-with-forward-and-back-propagation-sample-8f30af254b9e_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce premier schéma nous explique les quantités à l'oeuvre dans un perceptron multi-couches. Cette généralisation à plusieurs couches implique d'utiliser des notations matricielles et non plus vectorielles.\n",
    "\n",
    "<img src=\"https://miro.medium.com/proxy/1*95RWKvVOFvC8FDfnyeC1RA.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet autre schéma introduit la notion de gradient et la **_chain rule_** ou théorème de dérivation des fonctions composées (introduction du concept ici : https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_de_d%C3%A9rivation_des_fonctions_compos%C3%A9es)\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/proxy/1*4x1KP5r3nTW2IpUq3BBk1A.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, de manière schématique, on retrouve la méthode de mise à jour des gradients comme vu lors de la présentation du percetron mono-couche.\n",
    "\n",
    "<img src=\"https://miro.medium.com/proxy/1*UDslBPo8hSloFPPZoyCgPA.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation d'un perceptron multi-couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[2], 'GPU')\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge le data set MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    data_dir='datasets',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche les chiffres à classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAIFCAYAAACtXuUzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqp0lEQVR4nO3debRU1Zn38d9WuAyColEUUMEgoCiKDMmrIKI44ZCoIVGMwmqSVyMmEW3aaKedcGkrxCFpEDGYYPISEiKDAyiCE+m02HJRFCFEQWRUICqDQADZ7x9UTJ3z7KLOrap7a7jfz1pZYf/uPuduXZvi8dyHfZz3XgAAoH7br9gLAAAAxUdBAAAAKAgAAAAFAQAAEAUBAAAQBQEAAJDUoCaTnXP8HUUY3ntX7DXkg32NDDZ67w8r9iLywd5GSKbPbJ4QAEDYh8VeAFCXKAgAAAAFAQAAoCAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAquHLjQAAqBQdO3Y02ZgxYyLjfv36mTkTJkww2dChQ022Y8eO3BdXBDwhAAAAFAQAAICCAAAAiIIAAACIpkIAQD112mmnmeyss86KjL33Zs7gwYNN9sUXX5js+uuvj4x37txZ0yXWKZ4QAAAACgIAAEBBAAAAREEAAABEU2GdGzBggMkmT55ssmuvvdZkv/zlL2tlTUBNNWnSJDJ+5JFHzJymTZuabODAgSbbs2dP4RYGZHD++eeb7OGHHy7Y/YcMGWKyxYsXR8YPPfRQwb5fbeAJAQAAoCAAAAAUBAAAQBQEAABANBXWuSuvvNJkoZOwDjnkkLpYDpCVc85k48aNi4yvuuqqRPf6z//8T5O99dZbOa0LyCTU0DpixAiTNW/evFbXcdttt0XGNBUCAICSR0EAAAAoCAAAAD0Eta5t27aRcf/+/c2c6upqk/3ud7+rtTUBNdG5c2eTJekZ2Lx5s8n+9re/FWRNwL5MmTLFZD169DBZqH8rLtTj0rVr10TraNCgvP6I5QkBAACgIAAAABQEAABAFAQAAEAV3FQYOkwlJElTST5+/OMfR8ZVVVVmzvLly022atWqWlsTUBPf/va3c7pu5cqVJmNfo9C+//3vm6xv37453y/+eXzGGWeYOaGmxbPPPttk8abC9u3bmznLli2r6RJrDU8IAAAABQEAAKAgAAAAoiAAAACq4KbCUFNJ6E1TP/jBDyLjefPmFXQdXbp0yTqHt72hlN1www1Z5+zevdtkoTcbAvkaNGhQZDx69Ggzp2HDhonu9f7775vsvPPOi4y3bt1q5iQ9cbNRo0aRcejPJZoKAQBASaEgAAAAFAQAAICCAAAAqIKbCrdv326yUINf/BSqfJoKjzzyyKz337Jli5nzxBNP5Pw9gUJq0aKFyQ466KCs123YsMFkkyZNKsSSUI+1adPGZLfeemtknLSBcN26dSa79tprTbZixYpki8tBv379TPb444/X2verKZ4QAAAACgIAAEBBAAAAREEAAABUwU2F69evr/Pveemll5os3vAyf/58MyfU7AIUw4gRI3K67p133inwSlDfhJqyZ86cabKOHTvmdP+RI0ea7JVXXsnpXrk64YQT6vT71RRPCAAAAAUBAACgIAAAAKrgHoJDDjmkzr9n69ats86p659ZATXx/e9/P6frfv7znxd4JahvQgf05Poz99AbZCdMmJDTvQqpFNawLzwhAAAAFAQAAICCAAAAiIIAAACogpsKQ4cEOecKdv/QW7iuu+66rN/zV7/6VcHWABTLZ599FhnPnj27OAtBWTrvvPNMds455+R0r88//9xkl1xyick2bdqU0/1DQn+WJPnzJfS221LCEwIAAEBBAAAAKAgAAIAoCAAAgCqkqbBRo0Ymu+aaa0zmvTfZwIEDI+N27dqZOaFTD0866SSTNW/e3GRvvvlmZPzBBx+YOUAxdO3a1WTxt3NmMmbMmMh49+7dhVgSKlCLFi1MNn78eJOFPp9D4k2EgwcPNnNWrVqVbHEJVFVVmaxly5YmC63/iy++iIzXrFlTsHXVBp4QAAAACgIAAEBBAAAAREEAAABUIU2FV155pcmSvv64S5cukXGoWTBps0vIfffdFxnv2bMn53sBhTRy5EiTNWhgPxJ27dplsnhTIZBJqOk7yaviM3nmmWci42nTpuV8ryR+9KMfmaxv376Jrt2xY0dk/NxzzxViSbWGJwQAAICCAAAAUBAAAABREAAAAFVIU2HPnj1Ntm3bNpOFXj28du3ayPiTTz4xczZu3GiyJ598MtHann/++UTzgNrUtm1bk5166qkmCzXQvv/++yb76KOPCrMwVJw+ffpExk8//XTO9wrtx5kzZ+Z8v1xcdNFFOV8bP+WwR48eZs78+fNzvn+h8YQAAABQEAAAAAoCAACgCukhGDp0aKIsVwMGDDCZc85kU6dONdnmzZsLtg4gV8OHDzfZAQcckOja0AFGQCajR4+OjENvgU1q+fLlJps4cWLO90vizDPPjIx79eqV873iB9F9+umnOd+rLvCEAAAAUBAAAAAKAgAAIAoCAACgCmkqrG2htymGDsx444036mI5QI0lfTtbyIQJEwq2DlS+yZMnR8Z33XVXzvf6wx/+kO9y9umqq64y2Z133hkZ77///jnf/4477oiMly1blvO96gJPCAAAAAUBAACgIAAAAKIgAAAAoqkwkTPOOMNkoabCV199tS6WA2R18sknR8YdO3ZMdN306dNrYTWoTwr5Jsz42wIl6Xvf+15k3L17dzNn1apVJgs11sbfzJjpe8bFTyCUbDOlJD3wwANZ71VKeEIAAAAoCAAAAAUBAAAQBQEAABBNhUa3bt1M1qCB/df0wgsvmGzevHm1siagpuKvoG3YsGGi60aMGFEbywFyEnptd67228/+92+oOTDu448/NtmDDz5osp/97Ge5LayE8IQAAABQEAAAAAoCAAAgyYUO2Mk42bnkk8vU7NmzTdavXz+T7dq1y2TDhg0z2dixYwuyrlLmvXfFXkM+yn1fN2vWzGRLly6NjFu1amXmfPrppyYLzdu5c2ceqytr1d77HsVeRD6Ksbdbt24dGc+aNcvM6dy5c10t50vO2Y+pDRs2mOyxxx6LjB9//HEzZ8WKFQVbVzFk+szmCQEAAKAgAAAAFAQAAEAUBAAAQBxMZISaLEPZu+++a7Inn3yyVtYE7EvoTYah5sC4//mf/zFZPW4gRIGsXbs2Mg69UfCKK64w2W233Wayww8/PKc1TJgwwWTPPvusyV577TWTFfJtjeWGJwQAAICCAAAAUBAAAABREAAAANFUaBx//PEm+/zzz0122WWXmSx06hVQ2y6++OKcrhs/fnyBVwJYoRMxQye41odTXUsdTwgAAAAFAQAAoCAAAACiIAAAAOL1x8bGjRtNFmqK6dChQ10spyzw+uPiOvTQQ00WP0kz9Pu8ffv2Jgs10NZjvP4YFYnXHwMAgIwoCAAAAAUBAACgIAAAAKKpEAVAUyEqFE2FqEg0FQIAgIwoCAAAAAUBAACgIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAJIa1HD+Rkkf1sZCULbaFnsBBcC+Rgh7G5Uo476u0euPAQBAZeJHBgAAgIIAAABQEAAAAFEQGM65/Z1zbzrnnt3HnGHOuUGxbLhzzjvnDk2NuzjnJtTycoGsnHO/cs6td84tyjLvy33tnPu2c+5d59we51yPtDnsa5QM59z5zrmlzrn3nXO37GNe+t4+xDk32zn3Xur/D07l9X5vUxBYN0hakumLzrkGkoZI+l1adpSkcySt/EfmvX9H0pHOuaNrb6lAIhMknb+vCYF9vUjSZZLmps9jX6NUOOf2lzRGUn9JnSUNdM51DsyL7+1bJL3ove8g6cXUmL0tCoII59yRki6UNH4f086StMB7vzste0jSzZLif2XjGUlXFHSRQA157+dK+iTLtMi+9t4v8d4vzTCXfY1S8DVJ73vvl3vvd0r6vaRvBubFP7O/KemJ1K+fkHRJ2tx6vbcpCKIe1t4/2PfsY04vSdX/GDjnviFpjfd+YWDufEmnF3KBQC2J7Oss2NcoBW0krUobr05lcfG9fbj3fp0kpf6/ZdrX6vXepiBIcc5dJGm99z7bh2IrSRtS1zSV9FNJt2eYu15S64ItEqg9X+7rBNjXKAUukIUO1mFvJ0RB8E+9JH3DObdCex89neWc+3+BedslNU79ur2kYyQtTF13pKQFzrkjUl9vnJoPlLr0fZ0N+xqlYLWko9LGR0paG5gX39sfO+daSVLq/9enfa1e720KghTv/a3e+yO99+2092dIL3nvrwpMXSLp2NQ173jvW3rv26WuWy2pm/f+o9TcjtrbnAWUui/3dQLsa5SCNyR1cM4d45yr0t7P7acD8+J7+2lJg1O/HizpqbSv1eu9TUFQc89J6pNw7pmSZtTiWoCsnHOTJL0mqZNzbrVz7nuBaZF97Zy71Dm3WtKpkmY452alzWVfo+hSTYI/lDRLe//Qn+y9fzcwNf6ZfZ+kc5xz72nv3w67L+1r9Xpv8y6DHDjnpkm62Xv/3j7mNJL0qqTesb+RAJQk9jUqFXs7GQqCHDjnOmlvp+rcfczpIKmN9/6VOlsYkAf2NSoVezsZCgIAAEAPAQAAoCAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAoiAAAACiIAAAAKIgAAAAkhrUZLJzztfWQlC+vPeu2GvIB/saGWz03h9W7EXkg72NkEyf2TwhAICwD4u9AKAuURAAAAAKAgAAQEEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAJDUo9gIAlL/mzZub7PrrrzfZvffea7J169ZFxp07dzZzNm3alMfqgLBGjRqZ7M9//nNk/NWvftXMOfvss022YMGCwi2sSHhCAAAAKAgAAAAFAQAAEAUBAACQ5Lz3ySc7l3wy6g3vvSv2GvJRifs63ggVavr71re+ZbLGjRtnvVcoW7hwoZkzaNCgrOuUJOei26dVq1Zmzscff5zoXgVW7b3vUYxvXCiVuLcL6YgjjjDZ2rVrs163aNEik/Xs2dNkf//733NbWC3L9JnNEwIAAEBBAAAAKAgAAIAoCAAAgCr4pMKXX37ZZH379jXZ/fffHxnfcssttbUkoEZCp6gdc8wxJhs7dqzJTjnllMj4wAMPNHNq0lAcF28EPPnkk3O+F1Asd955Z07XhX4/HXbYYSZbvXp1TvcvFp4QAAAACgIAAEBBAAAAVIY9BPGfXUpSp06dTBb/Gaok7dmzx2Q33HBDZPzFF1+YOVOnTjVZ6OevS5cuNVncWWedZbLQwS8rVqww2cyZMyPjXbt2Zf1+KA+hPTB58mSThfZ1EvE3uEnSsmXLTDZjxgyTffbZZyabNWtWTusIWbNmTWS8Y8eOgt0b+IdLL73UZNdee63JkvTWLF682GTl1i8QwhMCAABAQQAAACgIAACAKAgAAIDKsKmwS5cuJnvzzTdzvl9VVVVkHDqYqFQOK/rTn/4UGYeaZD799NO6Wg7y0L9//8g41MwXsmXLFpOFDuEaNWpUZBxqKkzq6quvzjpn69atie4Veuviiy++GBlv2rQp2cKAGjjuuONyui7e9CpJQ4YMyXc5JYknBAAAgIIAAABQEAAAAFEQAAAASa4mbzxzzuX+erQctW3bNjIONVDF52SyefNmk8VPLzz44IPNnKT/jkKnKCa5NtREddBBB2W9/6OPPmrmDB06NOv3KzTvvf0HLyO1va9POOEEky1YsCAybtDA9vf+7//+r8kGDBhgslDTUyF17tzZZNddd11kHDql7cYbbzRZ6I1wzZo1i4y3b99e0yXWlmrvfY9iLyIfxfjMLlVLliwxWajRMP6Zfccdd5g5d999d+EWVgSZPrN5QgAAACgIAAAABQEAABAFAQAAUBmcVHjNNddExkkbCO+//36TPfzwwyaLNzCFXk9c2xYtWmSyv/71r1mvC536htJz0kknmSzURBh3wQUXmKwYJ1GGXvX6ox/9KDIeOHCgmRNqINy2bZvJSqiJEBUitB87dOiQ071WrVqV73LKBk8IAAAABQEAAKAgAAAAoiAAAAAqsabC3r17m2zYsGE53esXv/iFydavX5/1uqeeeiqn75ePY489NtG8+Ala5513npnTuHFjk+3YsSO3haEgTjnllJyu6969u8nmzJmT73Jqxb/9278lmvfAAw/U8koA6bbbbjPZfvsl++/fDRs2RMZTp04tyJrKAU8IAAAABQEAAKAgAAAAKrEegtDP+OM/E9+5c6eZM3r0aJMV4wCXXF155ZWJ5sXfdjhr1iwzh36B0jNx4kSTDR8+POt1L7zwQqL7P/vssyaL7/9169aZOdOnTzfZvHnzEn3PwYMHR8Zdu3Y1cz766COT3XnnnYnuD+Qj9NbapB566KHIOPSW3ErFEwIAAEBBAAAAKAgAAIAoCAAAgEqsqfC9994z2QknnBAZb9myxcxZs2ZNra2pLhx44IGJ5sUPJkJ5CL0t8MILL4yM77nnHjMntC+OOeaYrPcKiTekStKNN95osr/97W9Z7yVJBx10UGQc2psrV6402cknn2yyhQsXJvqeQMjVV19tspYtWya6duvWrSarz4dn8YQAAABQEAAAAAoCAAAgCgIAACDJ1aRRzTlHV1ueRowYYbLQm+KqqqpMFm+ovOSSS8ycV155Jee15cp7bzvWykip7uvmzZubLGlTYYsWLSLjUFNh6Pd+/ARCSTrssMNMFr9fPg2v77zzTmQc+v0we/bsnO+fh2rvfY9ifONCKdW9XUi/+c1vTBZqNAz57LPPTJbPKYflItNnNk8IAAAABQEAAKAgAAAAoiAAAAAqsZMKK9Hdd98dGd96661mTqjhK2T8+PGRcTEaCFF3Qqdyvv3224myJM4++2yTXXvttYmura6ujoxHjRpl5lxwwQUm69evn8lOOumkyPiPf/yjmdOtWzeTLV++POs6UXnir9q++OKLzZykTa4jR44sxJIqBk8IAAAABQEAAKAgAAAAoiAAAACiqTBnoUbA7373uyb713/916zXhbz00ksmu+WWWxKuDoi68847TRY6EbBJkyYm+/Of/2yy+ImGoQa/yZMnm6x3794mmzt3bmQceu1zs2bNTIb6qUOHDpFx/FXcNTFjxox8l1NReEIAAAAoCAAAAAUBAAAQPQSJtGvXzmR33XWXyUJv2EpyQMbSpUtN9i//8i8m2717d9Z7of5p2LChyaZPnx4Z9+/f38wJ7c2JEyea7Ic//KHJNm3aVIMV/lPogKG4RYsWmWzx4sU5fT9gX3r16mWyXA/6qgQ8IQAAABQEAACAggAAAIiCAAAASHJJ3wolSc655JPL1Iknnmiy+++/32Tnn39+TvefNm2ayYYPH26yFStW5HT/YvDeJzttqUSV6r4+4ogjTDZgwACTXX755VmvPfLII82c0L4OZdu3b9/nOjM54IADTDZ//nyTderUKTIOHfA1adKknNaQp2rvfY9ifONCKdW9nY8pU6ZExpdeemnO9/r8889N1rx585zvVy4yfWbzhAAAAFAQAAAACgIAACAKAgAAIE4qVJs2bSLjxx9/3Mzp0SP3vqL4KW9jx47N+V6oHPG3Cj7yyCNmTvyNglKyky8lac6cOZHxrbfeauY8+eSTie6Vqy5dupisY8eOJluzZk1kPHPmzFpbE8rfV7/61WIvoWLxhAAAAFAQAAAACgIAACAKAgAAIJoKdcMNN0TGPXv2NHNCjVxbt2412S233GKy8ePH57E6VIKvf/3rJhs9enRk3L17dzPHOXuY2IMPPmiye+65x2SffvppTZaYt6OPPtpkM2bMMFnon+nuu++OjHN9tTJQU6GTY+sznhAAAAAKAgAAQEEAAABUz3oI4j+rlGwPQahfIPQzzdBBL+PGjctjdahU3/rWt0zWrVu3yDjpgUNLliwxWejtbKGf6RfSaaedFhmHfj+0aNHCZMuWLTPZY489VrB1obKcccYZJjv++ONzutfbb79tskGDBuV0r0rFEwIAAEBBAAAAKAgAAIAoCAAAgCq4qTDU0HTllVearEGD6L+C0MEpv//9701GAyGSmjBhgskuvvjiyDj0FsCQUANe6BCigw8+ODIO7eukjYwh8fvt3LnTzAm9tTD0exDIpGnTpiarqqrK6V6hg7IQxRMCAABAQQAAACgIAACAKAgAAIAquKlw4MCBJmvXrl3W65YvX26ye++9txBLQj21ePFik3Xt2jUy7tOnj5nTq1cvk4X2cJMmTUw2YMCA5AtME1prdXW1yT766KPIePr06WbOvHnzcloD8A+zZ8822bBhwyLjc845x8wJnYj56quvFmxdlYonBAAAgIIAAABQEAAAAFEQAAAASa4mp5U553I/2qyO9e/f32Shk6ri//zXXXedmcPrWffNe2+PwSsj5bSvUaeqvfc9ir2IfLC3EZLpM5snBAAAgIIAAABQEAAAAFEQAAAAVfBJhS+99JLJXn/9dZN16tQp63UAAFQ6nhAAAAAKAgAAQEEAAABUwQcToe5wMBEqFAcToSJxMBEAAMiIggAAAFAQAAAACgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAACgmr/tcKOkD2tjIShbbYu9gAJgXyOEvY1KlHFf1+joYgAAUJn4kQEAAKAgAAAAFAQAAEAUBF9yznVyzr2V9r/NzrlhGeYOc84NSv36D2nXrHDOvZXKuzjnJtTZPwCQgXPuRufcu865Rc65Sc65xhnmpe/rrs65eal9Pd8597VUzr5GyXDO/co5t945tyjLvPS9/e3U74c9zrkeaXPq/d6mIEjx3i/13nf13neV1F3SNknT4vOccw0kDZH0u9R1l6ddN0XS1FT+jqQjnXNH180/AWA559pI+rGkHt77EyXtL+mKwLzIvpY0UtJdqX19e2rMvkapmSDp/H1NCOztRZIukzQ3fR57m4Igk36SlnnvQ39d5yxJC7z3u9ND55yT9B1Jk9LiZxT48AXqWANJTVIfjE0lrQ3Mie9rL+nA1K8Pil3DvkZJ8N7PlfRJlmmRve29X+K9X5phbr3e2xQEYVco+gd7ul6SqgP56ZI+9t6/l5bNT+VAUXjv10j6maSVktZJ2uS9fyEwNb6vh0ka5Zxblbr+1rSvsa9RTjJ9ZofU671NQRDjnKuS9A1Jf8wwpZWkDYF8oGwRsV5S68KtDqgZ59zBkr4p6Rjt3YsHOOeuCkyN7+vrJN3ovT9K0o2SHk/7Gvsa5STTZ3ZIvd7bFARWf+19vPRxhq9vlxRpyko9ir1M0h9icxun5gPFcrakD7z3G7z3u7S3x+W0wLz4vh6cmivtLY6/lvY19jXKifnM3od6vbcpCKzQf+mnWyLp2Fh2tqS/eO9Xx/KO2tvAAhTLSkn/xznXNNXn0k9793BcfF+vlXRG6tdnSUr/URj7GuUk9JmdSb3e2xQEaZxzTSWdo3/+l1HIc5L6xLJMPQdnSppRmNUBNee9f13Sk5IWSHpHe3/PPxaYGt/X/1fSA865hZLulXRN2tfY1ygJzrlJkl6T1Mk5t9o5973AtMjeds5d6pxbLelUSTOcc7PS5tbrvc27DHLgnJsm6eZYA2F8TiNJr0rqHf8bCUApYl+jUrG3k6EgyIFzrpOkw1N/5SXTnA6S2njvX6mzhQF5YF+jUrG3k6EgAAAA9BAAAAAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACCpQU0mO+d8bS0E5ct774q9hnywr5HBRu/9YcVeRD7Y2wjJ9JnNEwIACPuw2AsA6hIFAQAAoCAAAAAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEBSg2IvoBD+67/+y2Tdu3dPdO3zzz8fGX/44YdmzkcffWSyWbNmJVwdAKCSHHfccSZ76623TPbGG29ExqeffnptLakgeEIAAAAoCAAAAAUBAAAQBQEAAFAZNBU2atQoMh4zZoyZM2TIkJzvf+qpp0bG3nszZ8+ePSabP3++yW6//XaTvfDCCzmvDQBQenr37m2y/fff32QnnnhiZNy+fXszZ9myZYVbWJ54QgAAACgIAAAABQEAABAFAQAAUBk0Fd58882RcT4NhCGhJsK4/fazddPXvvY1k4UaHgcOHBgZh5oRgVLRp08fk/3iF78wWadOnSLjm266ycwZO3Zs4RYGFEn//v1NFmogb9DA/nG6bdu2yHjHjh2FW1gt4AkBAACgIAAAABQEAABAZdBD0Lp166xzpk6darKFCxeabOvWrSb77W9/GxnHD0KSpIkTJ5rstNNOM1no0InHHnssMu7Zs6eZ88UXX5gMCGnWrJnJdu/ebbL4XowfkCKF93Coh6BLly5Z1xU/4EuihwDlKX7A0NChQ82co446ymShz/EXX3wxMl6zZk2eq6tdPCEAAAAUBAAAgIIAAACIggAAAKgMmgrjjUkrV640c0aOHGmyQjbq9e3b12TPP/+8yc4991yTde3aNTL+wQ9+YOaEDjRCZWvatGlkPHPmzETX7dy502THHnusyQ4//PDIuHHjxmaOc85kSQ7qCtmyZUtO1wGlZsSIEZHxRRddlOi6N954w2SDBg0qyJrqCk8IAAAABQEAAKAgAAAAoiAAAACSXE2aiJxzuXUcVaDevXubbM6cOSarqqqKjNevX2/mhN6cGGqeLFXee9udVkaKsa+/8pWvRMahfZFP01/8rWqh0wx//etfZ12XJF1++eUmi5/mFnoj4o033ph1nSWu2nvfo9iLyAef2ft23HHHmay6ujoyjjcAS+Gm9Ysvvthkzz33XB6rq7kePex2Db1hN9NnNk8IAAAABQEAAKAgAAAAoiAAAAAqg5MKS9V///d/m2zUqFEm++lPfxoZt2zZ0sxp166dycqpqRA1Fz/Z78ILLyzo/VesWBEZb9682cxZu3ZtonuFml7jpyOG7g+UklBz4B133JFoXtykSZNMVtcNhCHbtm3L63qeEAAAAAoCAABAQQAAAERBAAAARFNhQT311FMmizcVhnTp0sVkc+fOLciaUJrirzEOvU67GFq0aGGyUJNV/BTFeBMjUGpCJwleccUVWa/75JNPTDZu3LiCrKnQFi9enNf1PCEAAAAUBAAAgIIAAACIHoKSEPrZ1qOPPmqy0Bu2gELq1KmTyVq3bm2y+FsXzzzzTDMn9DZFoC707dvXZE888USia+N7+6abbjJzQgfTVQKeEAAAAAoCAABAQQAAAERBAAAARFNhQW3YsMFkGzdujIwPPfRQMyf+5jhJqqqqMtn27dvzWB2QXeiQrCTeeeedAq8EyN3tt99uskaNGiW6dvTo0ZFx0mbESsATAgAAQEEAAAAoCAAAgCgIAACAyrCpMPQ2ttBJaiG7d+822V//+td8l/Slww47zGShJsK4hx56yGQ0EKIYcm0qLOTvI6AmrrvuOpP17t070bUffvihyf7jP/4j7zWVK54QAAAACgIAAEBBAAAAREEAAABUBk2F/fv3j4xDDXgdO3ZMdK+dO3ea7K677oqMZ86caeYsXLgw0f2/+c1vJpoXxylvyEdo38WbAz/44AMz57vf/a7JjjvuuJzWED/dTZK6d+9ustAJckBNHH744ZHxT37yEzOnYcOGJgs1lY8aNcpkmzdvzmN15Y0nBAAAgIIAAABQEAAAAFEQAAAAlUFT4VNPPRUZN2iQ+5JDrxS+5557IuM77rjDzHnmmWdMNmPGDJPdfPPNWdewa9cuk/3973/Peh0gSePHjzfZ5ZdfbrIDDjgg672ccybz3idaR7xBN/R7C8hX6PM+/jritm3bJrpXqHl7zJgxuS2sQvGEAAAAUBAAAAAKAgAAIMkl/ZmhJDnnkk8ukPiBKkl/XrRu3TqThX6GdO655+a2sBwtWbLEZCeccEKdrkGSunXrZrKjjjoqMo73b2Tivbc/jC4jxdjXuWrXrp3Jxo4da7L27dtHxhs3bjRzQj0ERx99tMmOOOIIk82aNSsyDvUxbNmyxWRlptp736PYi8hHOe3tkK5du5rszTffzHpd6BCi73znOyabNm1aTusqd5k+s3lCAAAAKAgAAAAFAQAAEAUBAABQGRxMNGLEiMh43LhxZk7o8Irq6mqTXXPNNSZr3LhxZPynP/3JzGnTpk3WdSbVoUMHk61Zs8ZkixcvNlnnzp0Lto4WLVqYLN5k1rRp04J9PxTGihUrTBZ/I6gkNW/ePDJO2uD30ksvmSzUVBh/K2IFNBCiBN122205Xffzn//cZPW1gbAmeEIAAAAoCAAAAAUBAAAQBQEAAFAZNBX++te/joxDTVW//OUvTXbRRReZbO3atSZ77bXXIuNDDjmkhiusmVADZKtWrRJluVq5cqXJpk6darIHHnigYN8TxZWkyS906mHPnj0T3b9hw4Y1XRKwTz162EMhQw2zSUyfPj3P1dRPPCEAAAAUBAAAgIIAAACIggAAAKgMmgrjXn75ZZPddNNNJhs1apTJQk1Up556atbvuXPnTpOFXsF5zz33mOwvf/lL1vuHDBkyxGRVVVWRceg0xjfeeMNkn332mclCr8NF/XL88cebLOnplFOmTCn0clDPDR8+3GRNmjTJet2cOXNM9vrrrxdkTfUNTwgAAAAFAQAAoCAAAAAqwx6CkKeffjpR1rVrV5OddNJJWe8/d+5ck4UOSCqkf//3f6/V+wOhnpr4Gy8zWbduXYFXg/qkZcuWJkvSzxVy3333mWzXrl053au+4wkBAACgIAAAABQEAABAFAQAAEAV0lSY1FtvvZUoA+qDQw891GTe+0TXhg4IA5I6+OCDTXb00UfndK89e/bkuxyk8IQAAABQEAAAAAoCAAAgCgIAAKB61lQI4J86duyYaF7oVM633367wKtBffLBBx+Y7JFHHjHZ0KFDTfbJJ59ExqtWrSrcwuo5nhAAAAAKAgAAQEEAAABEQQAAAERTIYAsPv/8c5Pt2LGjCCtBpdi5c6fJrr/++kQZag9PCAAAAAUBAACgIAAAAKIgAAAAoqkQQBZTpkwp9hIA1AGeEAAAAAoCAABAQQAAACQ5733yyc4ln4x6w3vvir2GfLCvkUG1975HsReRD/Y2QjJ9ZvOEAAAAUBAAAAAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAoJq/7XCjpA9rYyEoW22LvYACYF8jhL2NSpRxX9fo6GIAAFCZ+JEBAACgIAAAABQEAABAFARfcs4d5Zx72Tm3xDn3rnPuhn3MHeacG5T69Sjn3F+cc28756Y551qk8i7OuQl1s3ogzDnXyTn3Vtr/NjvnhmWYm76v/5B2zQrn3FupnH2NkuGc+5Vzbr1zblGWeel7+9upz/g9zrkeaXPq/d6mqTDFOddKUivv/QLnXHNJ1ZIu8d4vjs1rIGmBpG7e+93OuXMlvZT69f2S5L3/SWruHElDvPcr6/QfBghwzu0vaY2kr3vvP4x9LbKvY197QNIm7/2I1Jh9jZLgnOsjaauk33jvT8wwJ/6ZfbykPZLGSRruvZ+fNrde722eEKR479d57xekfr1F0hJJbQJTz5K04B8fmt77F9I+QOdJOjJt7jOSrqi9VQM10k/SsngxkBLZ1//gnHOSviNpUlrMvkZJ8N7PlfRJlmnxz+wl3vulGebW671NQRDgnGsn6RRJrwe+3Et7nx6EDJH0XNp4vqTTC7o4IHdXKPoHe7pM+/p0SR97799Ly9jXKCf7+syOq9d7m4IgxjnXTNIUScO895sDU1pJ2hC47qeSdkuamBavl9S6NtYJ1IRzrkrSNyT9McOU4L6WNFC2iGBfo5xk2tsh9Xpv1/SkwormnGuovcXARO/91AzTtktqHLtusKSLJPXz0aaMxqn5QLH1197Hph9n+HpoXzeQdJmk7rG57GuUE7O396Fe720KgpTUz0ofl7TEe//gPqYukXRs2nXnS/qJpDO899ticztK2mf3K1BHQv+lny6yr1POlvQX7/3qWM6+RjkJ7e1M6vXe5kcG/9RL0tWSzkr761YXBOY9J6lP2ni0pOaSZqeueTTta2dKmlFrKwYScM41lXSOpExPvSS7r6XMPQfsa5QE59wkSa9J6uScW+2c+15gWmRvO+cudc6tlnSqpBnOuVlpc+v13uavHebAOTdN0s2xRqv4nEaSXpXUO965DZQi9jUqFXs7GQqCHDjnOkk6PPVXXjLN6SCpjff+lTpbGJAH9jUqFXs7GQoCAABADwEAAKAgAAAAoiAAAACiIAAAAKIgAAAAkv4/0X4Lb4UKPpwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.visualization.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis les informations d'intérêt sur ce data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    full_name='mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    The MNIST database of handwritten digits.\n",
      "    \"\"\",\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    data_path='datasets/mnist/3.0.1',\n",
      "    download_size=11.06 MiB,\n",
      "    dataset_size=21.00 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment, on convertit en format _floattant_. Pas besoin de normaliser car les images sont en noir et blanc !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image: tf.Tensor, label: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Conversion et normalisation.\"\"\"\n",
    "    img = tf.cast(image, tf.float32) / 255.\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on applique les transformations de manière parallélisée.\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "# on cache de manière à faire la transformation précédente une seule fois\n",
    "# au début de l'entraînement.\n",
    "ds_train = ds_train.cache()\n",
    "# on permute aléatoirement les exemples.\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "# on constitue des batches de taille 128.\n",
    "ds_train = ds_train.batch(128)\n",
    "# enfin, on demande à Tensorflow de traiter la donnée\n",
    "# même si le GPU est en train de calculer (système de queue).\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même manière, on construit l'échantillon de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on implémente un MLP à 1 couche cachée de taille : 784 (entrée), 128 (cachée) et 10 (sortie).\n",
    "Pour cela, on aplatit les images en un seul et même vecteur de taille $28\\times 28=784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.7781 - sparse_categorical_accuracy: 0.6314 - val_loss: 0.6085 - val_sparse_categorical_accuracy: 0.8626\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5422 - sparse_categorical_accuracy: 0.8679 - val_loss: 0.3851 - val_sparse_categorical_accuracy: 0.8995\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3760 - sparse_categorical_accuracy: 0.8987 - val_loss: 0.3255 - val_sparse_categorical_accuracy: 0.9114\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3289 - sparse_categorical_accuracy: 0.9066 - val_loss: 0.2975 - val_sparse_categorical_accuracy: 0.9163\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3033 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.2796 - val_sparse_categorical_accuracy: 0.9197\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2800 - sparse_categorical_accuracy: 0.9181 - val_loss: 0.2671 - val_sparse_categorical_accuracy: 0.9233\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2694 - sparse_categorical_accuracy: 0.9218 - val_loss: 0.2528 - val_sparse_categorical_accuracy: 0.9257\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2580 - sparse_categorical_accuracy: 0.9260 - val_loss: 0.2454 - val_sparse_categorical_accuracy: 0.9279\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2391 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.2349 - val_sparse_categorical_accuracy: 0.9317\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2337 - sparse_categorical_accuracy: 0.9321 - val_loss: 0.2244 - val_sparse_categorical_accuracy: 0.9351\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2220 - sparse_categorical_accuracy: 0.9358 - val_loss: 0.2161 - val_sparse_categorical_accuracy: 0.9373\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2131 - sparse_categorical_accuracy: 0.9388 - val_loss: 0.2069 - val_sparse_categorical_accuracy: 0.9397\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2040 - sparse_categorical_accuracy: 0.9421 - val_loss: 0.1991 - val_sparse_categorical_accuracy: 0.9408\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1940 - sparse_categorical_accuracy: 0.9449 - val_loss: 0.1910 - val_sparse_categorical_accuracy: 0.9429\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1836 - sparse_categorical_accuracy: 0.9481 - val_loss: 0.1830 - val_sparse_categorical_accuracy: 0.9455\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1739 - sparse_categorical_accuracy: 0.9499 - val_loss: 0.1739 - val_sparse_categorical_accuracy: 0.9480\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1681 - sparse_categorical_accuracy: 0.9511 - val_loss: 0.1688 - val_sparse_categorical_accuracy: 0.9481\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1568 - sparse_categorical_accuracy: 0.9545 - val_loss: 0.1621 - val_sparse_categorical_accuracy: 0.9521\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1501 - sparse_categorical_accuracy: 0.9575 - val_loss: 0.1570 - val_sparse_categorical_accuracy: 0.9547\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1480 - sparse_categorical_accuracy: 0.9571 - val_loss: 0.1528 - val_sparse_categorical_accuracy: 0.9541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f08ec5df610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)), # entrée\n",
    "  tf.keras.layers.Dense(128, activation='relu'), # couche cachée\n",
    "  tf.keras.layers.Dense(10, activation='softmax') # couche de sortie\n",
    "])\n",
    "# on définit un optimiseur pour l'entraînement.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "# on entraîne sur 20 epochs.\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=20,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nombre de paramètres requis pour l'entraînement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison avec un petit réseau convolutif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Faisons la même chose mais entraînons un réseau convolutif...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 3s 4ms/step - loss: 1.8394 - sparse_categorical_accuracy: 0.3357 - val_loss: 0.5138 - val_sparse_categorical_accuracy: 0.8386\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4919 - sparse_categorical_accuracy: 0.8468 - val_loss: 0.3565 - val_sparse_categorical_accuracy: 0.8880\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3555 - sparse_categorical_accuracy: 0.8890 - val_loss: 0.2852 - val_sparse_categorical_accuracy: 0.9133\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2945 - sparse_categorical_accuracy: 0.9089 - val_loss: 0.2655 - val_sparse_categorical_accuracy: 0.9155\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2575 - sparse_categorical_accuracy: 0.9196 - val_loss: 0.2305 - val_sparse_categorical_accuracy: 0.9267\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2251 - sparse_categorical_accuracy: 0.9303 - val_loss: 0.1946 - val_sparse_categorical_accuracy: 0.9371\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2054 - sparse_categorical_accuracy: 0.9354 - val_loss: 0.1847 - val_sparse_categorical_accuracy: 0.9423\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1809 - sparse_categorical_accuracy: 0.9435 - val_loss: 0.1637 - val_sparse_categorical_accuracy: 0.9489\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1703 - sparse_categorical_accuracy: 0.9467 - val_loss: 0.1562 - val_sparse_categorical_accuracy: 0.9514\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1584 - sparse_categorical_accuracy: 0.9502 - val_loss: 0.1445 - val_sparse_categorical_accuracy: 0.9532\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1501 - sparse_categorical_accuracy: 0.9537 - val_loss: 0.1406 - val_sparse_categorical_accuracy: 0.9545\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1463 - sparse_categorical_accuracy: 0.9551 - val_loss: 0.1324 - val_sparse_categorical_accuracy: 0.9571\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1316 - sparse_categorical_accuracy: 0.9584 - val_loss: 0.1274 - val_sparse_categorical_accuracy: 0.9580\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1253 - sparse_categorical_accuracy: 0.9613 - val_loss: 0.1176 - val_sparse_categorical_accuracy: 0.9639\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1217 - sparse_categorical_accuracy: 0.9622 - val_loss: 0.1138 - val_sparse_categorical_accuracy: 0.9629\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1198 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.1143 - val_sparse_categorical_accuracy: 0.9636\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1131 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.1068 - val_sparse_categorical_accuracy: 0.9656\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1066 - sparse_categorical_accuracy: 0.9658 - val_loss: 0.1095 - val_sparse_categorical_accuracy: 0.9653\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1006 - sparse_categorical_accuracy: 0.9685 - val_loss: 0.1008 - val_sparse_categorical_accuracy: 0.9683\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1015 - sparse_categorical_accuracy: 0.9680 - val_loss: 0.1022 - val_sparse_categorical_accuracy: 0.9658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f087c1a06d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # première couche de convolutions en 2D.\n",
    "    tf.keras.layers.Conv2D(32, (2, 2), activation='relu'),\n",
    "    # premier pooling.\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    # deuxième couche de convolutions en 2D.\n",
    "    tf.keras.layers.Conv2D(32, (2, 2), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    # troisième couche de convolutions en 2D.\n",
    "    tf.keras.layers.Conv2D(32, (2, 2), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    # quatrième couche de convolutions en 2D.\n",
    "    tf.keras.layers.Conv2D(32, (2, 2), activation='relu'),\n",
    "    # on transforme la sortie en un vecteur 1D.\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # et on applique un softmax.\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "# on définit un optimiseur pour l'entraînement.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "# on entraîne sur 20 epochs.\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=20,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nombre de paramètres requis pour l'entraînement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_35 (Conv2D)           (None, 27, 27, 32)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 12, 12, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 5, 5, 32)          4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 1, 1, 32)          4128      \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 12,874\n",
      "Trainable params: 12,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # 12 874 contre 101 770 précédemment..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici semble se dessiner un premier indice de pourquoi plus personne n'utilise de MLP pour analyser les images, mais plutôt des réseaux convolutifs : **le nombre de paramètres est plus faible** tandis que **la convolution semble également mieux capturer l'information importante pour classer les images**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution : motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien qu'efficaces pour le traitement d'images, les perceptrons multicouches (MLP) ont des difficultés à gérer des images de grande taille, en raison de la croissance exponentielle du nombre de connexions avec la taille de l'image, du fait que chaque neurone est « totalement connecté » à chacun des neurones de la couche précédente et suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **La conception des réseaux de neurones convolutifs suit la découverte de mécanismes visuels dans les organismes vivants.**\n",
    "\n",
    "Début 1968 (Hubel), des travaux ont montré chez l'animal que le cortex visuel contient des arrangements complexes de cellules de la rétine dites **ganglionnaires**. Chaque cellule ganglionnaire de la rétine signale par une augmentation de sa fréquence de décharge l’apparition d’un **contraste chromatique ou de luminance** dans une petite fenêtre du champ visuel de l'oeil humain. Cette région d'activation est appelée le **champ récepteur** du neurone (Bullier, 2002 ; Hubel, 1994 ; Fitzpatrick, 2000). Il s’agit de la zone du champ visuel qui influence l’activité d’une cellule. \n",
    "\n",
    "Il existe 120 millions de bâtonnets, 6 millions de cônes et 1,5 millions de cellules ganglionnaires (Schwartz, 1999 ; Wandell, 1995). Les cellules ganglionnaires signalent le résultat d’une comparaison entre **la quantité de lumière atteignant une région de la rétine et la quantité de lumière qui atteint le pourtour de cette région**. Les champs récepteurs des cellules dites naines et en parasol fait que ces derniers répondent de **façon préférentielle au contraste et pas à une luminance absolue**. Ceci implique que l’information de surface est perdue : les cellules répondent si un contour (un contraste noir/blanc) correctement placé atteint leur champ récepteur (la limite entre deux formes est indiquée par une différence de contraste ; Hubel, 1994 ; Schwartz, 1999 ; Wandell, 1995). **Les bords d’une forme visuelle sont donc encodés dès les premiers niveaux de traitement.**\n",
    "\n",
    "Le champ récepteur d’une cellule ganglionnaire comprend deux régions concentriques : le **centre** qui est la région la plus sensible et le **pourtour** qui a une action antagoniste à celle du centre. Deux types fonctionnels de neurones sont distingués : les neurones à centre-on et à centre-off. **L’activité des neurones à centre-on s’accroît lorsque le contraste de luminance augmente entre le centre et le pourtour du champ récepteur. Ainsi, une cellule à centre-on décharge avec une fréquence plus élevée lorsqu’une tache de lumière est présentée précisément au centre du champ récepteur et que son pourtour n’est pas éclairé. La réponse de cette cellule est faible ou nulle lorsque son champ récepteur est totalement éclairé.**\n",
    "\n",
    "<img src=\"http://theses.univ-lyon2.fr/documents/getpart.php?id=570&file=pic02.gif\" width=\"400\">\n",
    "Palmer et al., 1999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implications sur le traitement d'une image**\n",
    "\n",
    "Dans le cadre de la reconnaissance d'image, on va donc paver cette dernière de multiple zones régulières ou tuiles. **Chaque tuile sera traitée individuellement par un neurone artificiel (une cellule ganglionnaire, qui effectue une opération de filtrage classique en associant un poids à chaque pixel de la tuile).**\n",
    "\n",
    "**Tous les neurones ont les mêmes paramètres de réglage, pour rendre compte du caractère uniforme du traitement de l'entrée lumineuses des cellules ganglionnaires**. Le fait d'avoir le même traitement (mêmes paramètres), légèrement décalé pour chaque champ récepteur, s'appelle une convolution. Cette strate de neurones avec les mêmes paramètres est appelée « noyau de convolution ».\n",
    "\n",
    "Cette animation permet de comprendre l'opération d'une convolution et le concept de noyau. Ici, on choisit un noyau qui va venir \"scanner\" les différentes régions de l'image.\n",
    "\n",
    "$$ \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 2 \\\\\n",
    "2 & 2 & 0 \\\\\n",
    "0 & 1 & 2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/535/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" width=\"200\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Autre example avec un image RVB (en 3D).\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*ciDgQEjViWLnCbmX-EeSrA.gif\" width=\"800\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En pratique, les **champs récepteurs se chevauchent** (comme sur l'image) afin d'obtenir une meilleure représentation de l'image originale ainsi qu'une meilleure cohérence du traitement au fil des couches de traitement. Le chevauchement est défini par le pas (décalage entre deux champs récepteurs adjacents) ou **_stride_**. \n",
    "\n",
    "\n",
    "\n",
    "Les pixels d'une tuile sont analysés globalement. Dans le cas d'une image en couleur, un pixel contient 3 entrées (rouge, vert et bleu), qui seront traitées globalement par chaque neurone. Donc l'image peut être considérée comme un volume, et notée par exemple 30 × 10 × 3 pour 30 pixels de largeur, 10 de hauteur et 3 de profondeur correspondant aux 3 canaux rouge, vert et bleu. De manière générale, on parlera de « volume d'entrée ». "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation de certains noyaux de convolution de référence pour fixer la notion\n",
    "\n",
    "https://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on l'a vu sur le site précédent, un noyau de convolution va **analyser une caractéristique de l'image d'entrée**. \n",
    "\n",
    "**Pour analyser plusieurs caractéristiques, on va empiler des strates de noyaux de convolution indépendants, chaque strate analysant une caractéristique de l'image.**\n",
    "\n",
    "**L'ensemble des strates ainsi empilées forme la « couche de traitement convolutif »** ou **convolutional layer**, qu'il faut voir en fait comme un volume (souvent appelé « volume de sortie »). Le nombre de strates de traitement (ou **filters**) s'appelle la profondeur de la couche de convolution (à ne pas confondre avec la profondeur d'un réseau de neurones convolutifs qui compte le nombre de couches de convolution).\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1992/0*U5Ej4yTMTQBfyr_N.png\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1898/0*R52u4buyPfabCeyV.png\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://images4.programmersought.com/795/e2/e2809fb699bfbab4ea40ff1fea956813.png\" width=\"400\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour résumé, l'essentiel d'un _CNN_ est assuré par les neurones de traitement, qui traitent une portion limitée de l'image (appelée « **champ réceptif** ») au travers d'une fonction de **convolution**. **Les réseaux de neurones convolutifs visent à limiter le nombre d'entrées tout en conservant la forte corrélation « spatialement locale » des images naturelles.**\n",
    "\n",
    "Les 3 grands avantages de la convolution sont les suivants :\n",
    "\n",
    "- **Connectivité locale** : grâce au champ récepteur qui limite le nombre d'entrées du neurone (par exemple $3\\times 3$), les réseaux de neurones convolutifs assurent ainsi que les filtres produisent **la réponse la plus forte à un motif d'entrée spatialement localisé, ce qui conduit à une représentation parcimonieuse de l'entrée. De plus, le nombre de paramètres à estimer étant réduit, leur estimation (statistique) est plus robuste pour un volume de données fixé (comparé à un MLP).** Au travers du processus d'optimisation, chaque filtre va se spécialiser dans une caractéristique spéciale de l'image; caractéristique qui s'avère pertinente pour la prédiction finale.\n",
    "\n",
    "- **Poids partagés** : dans les réseaux de neurones convolutifs, les paramètres de filtrage d'un neurone (pour un champ récepteur donné) sont identiques pour tous les autres neurones d'un même noyau (traitant tous les autres champs récepteurs de l'image), ce qui ccupe moins d'espace en mémoire. \n",
    "\n",
    "- **Invariance à la translation** : comme tous les neurones d'un même noyau (filtre) sont identiques, le motif détecté par ce noyau est indépendant de localisation spatiale dans l'image.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profondeur, stride et padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trois hyperparamètres permettent de dimensionner le volume de la couche de convolution (aussi appelé volume de sortie) : la profondeur (**depth**), le pas (**stride**) et la marge (**pad**).\n",
    "\n",
    "- **Depth** : nombre de noyaux de convolution (ou nombre de neurones associés à un même champ récepteur).\n",
    "- **Stride** : contrôle le chevauchement des champs récepteurs. Plus le pas est petit, plus les champs récepteurs se chevauchent et plus le volume de sortie sera grand.\n",
    "- **Pad** : La marge (à 0) ou zero padding : parfois, il est commode de mettre des zéros à la frontière du volume d'entrée. Cette marge permet de contrôler la dimension spatiale du volume de sortie. En particulier, il est parfois souhaitable de conserver la même surface que celle du volume d'entrée.\n",
    "\n",
    "Illustration d'un _stride_=2 :\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/395/1*1VJDP6qDY9-ExTuQVEOlVg.gif\" width=\"200\">\n",
    "\n",
    "Illustration du _padding_ : \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/395/1*nYf_cUIHFEWU1JXGwnz-Ig.gif\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre concept important des CNNs est le **pooling**, ce qui est une forme de sous-échantillonnage (ou _downsampling_) de l'image. Le pooling réduit la taille spatiale d'une image intermédiaire, réduisant ainsi la quantité de paramètres et de calcul dans le réseau. **Il est donc fréquent d'insérer périodiquement une couche de pooling entre deux couches convolutives successives d'une architecture de réseau de neurones convolutifs pour réduire le sur-apprentissage.**\n",
    "\n",
    "**Très important : l'opération de pooling crée aussi une forme d'invariance par translation et rotation.** En effet, celui-ci permet de garder les features les plus saillantes sur l'image (ou ses représentations intermédiaires), peu importe la localisation initiale de celle-ci. \n",
    "\n",
    "La forme la plus courante est une couche de _pooling_ de taille 2 × 2 (largeur/hauteur) et comme valeur de sortie la valeur maximale en entrée (_max-pooling_). On parle dans ce cas de « Max-Pool 2x2 » (compression d'un facteur 4). Il est aussi possible d'éviter la couche de pooling30 mais cela implique un risque de sur-apprentissage plus important. \n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2400/1*KQIEqhxzICU7thjaQBfPBQ.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Couche d'activation non-linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convolution est une opération purement linéaire (produit de matrice). De fait, sans activation non-linéaire, il serait bien difficile de modéliser des relations non-linéaires et donc relativement complexes, entre les images d'entrée et la sortie (_i.e._ la prédiction). \n",
    "\n",
    "Ces dernières années c'est imposé l'activation dite ReLU pour **Rectified Linear Unit** (2010, Nair, Vinod and Hinton, Geoffrey E.  Rectified linearunits  improve  restricted  Boltzmann  machines. Cette fonction d'activation présente l'immense intérêt d'agir à la fois comme un moyen efficace de **propager les gradients** (on parle de **vanishing gradient problem**) car la gradient de cette fonction ne sature pas (c'est la fonction idendité); mais également comme un bon régulariseur car, comme la fonction de Heaviside, les neurones ne s'activent que lorsque la somme pondérée qui leur est transmise est positive.\n",
    "\n",
    "\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-07-at-6-18-19-pm.png?w=748\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réseau convolutif : dernières étapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Couches denses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Très généralement, on utilise les différentes couches précédemment citées dans l'ordre suivant : convolution, activation, pooling.\n",
    "Au fur et à mesure que l'image initiale est traitée, la dimension de sa représentation diminue en longueur et largeur, jusqu'à atteindre une taille $h_f \\times w_f \\times d_f$. Dans l'exemple que nous avions pris plus haut, l'ensemble des différentes convolutions et opérations de _pooling_ avait mené à une représentation de l'image de taille $1 \\times 1 \\times 32$, c'est-à-dire $32$ valeurs. \n",
    "\n",
    "Dans le cadre ici d'une **classification**, l'avant dernière étape consiste à utiliser des couches entièrement connectées, c'est-à-dire un perceptron, avec plus ou moins de couches ! **Les neurones dans une couche entièrement connectée ont des connexions vers toutes les sorties de la couche précédente (comme on le voit régulièrement dans les réseaux réguliers de neurones)**. Le premier réseau de neurones convolutif profond AlexNet (Lecun, 1988) entraîné selon la méthode de _backpropagation_ (dans le cadre de la classification MNIST); **plus de 90 % des paramètres à apprendre sont dus aux 3 couches « complètement connectées » les plus profondes, et le reste concerne les (5) couches convolutives.**\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png\" width=\"800\">\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3684/1*vXBvV_Unz3JAxytc5iSeoQ.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_35 (Conv2D)           (None, 27, 27, 32)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 12, 12, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 5, 5, 32)          4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 1, 1, 32)          4128      \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 12,874\n",
      "Trainable params: 12,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Couche softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La couche de perte spécifie comment l'entrainement du réseau pénalise l'écart entre la prédiction et le label réel attendu. Elle est normalement la dernière couche dans le réseau. Diverses fonctions de perte adaptées à différentes tâches peuvent y être utilisées.\n",
    "\n",
    "La perte **softmax** est utilisée pour prédire une seule classe parmi *K* classes mutuellement exclusives. \n",
    "\n",
    "\n",
    "<img src=\"https://deepnotes.io/public/images/softmax.png\" width=\"400\">\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1906/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction de coût"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La perte par entropie croisée sigmoide est utilisée pour prédire $K$ valeurs de probabilité indépendante dans [ 0 , 1 ] {\\displaystyle [0,1]} {\\displaystyle [0,1]}. La perte euclidienne est utilisée pour régresser vers des valeurs réelles dans $[-\\infty ,\\infty ]$. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/919/1*ETtY7KCrzAlOmLeyDWE4Xg.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat Tensorflow : 1.012184739112854\n",
      "Résultat ad-hoc: 1.0121847560247488\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# y_true = [0, 1, 2]\n",
    "y_true = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "y_pred = np.array([\n",
    "    [0.2, 0.5, 0.3],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.4, 0.3, 0.3]\n",
    "])\n",
    "# Implémentation TensorFlow.\n",
    "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "print(f'Résultat Tensorflow : {cce(y_true, y_pred).numpy()}')\n",
    "ours = -np.diag((y_true * np.log(y_pred))).mean()\n",
    "print(f'Résultat ad-hoc: {ours}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on refait l'exercice avec les mêmes classes prédites mais des probabilités plus éclatées pour les cas où il y a des erreurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat Tensorflow : 1.9466382265090942\n",
      "Résultat ad-hoc: 1.914868156392152\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# y_true = [0, 1, 2]\n",
    "y_true = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "y_pred = np.array([\n",
    "    [0.04, 0.95, 0.01], # erreur\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.9, 0.1, 0.1] # erreur\n",
    "])\n",
    "# Implémentation TensorFlow.\n",
    "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "print(f'Résultat Tensorflow : {cce(y_true, y_pred).numpy()}')\n",
    "ours = -np.diag((y_true * np.log(y_pred))).mean()\n",
    "print(f'Résultat ad-hoc: {ours}')\n",
    "# L'erreur est plus élevée : quand on se trompe, on se trompe \"trop\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN : https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html\n",
    "- MLP : https://www.cs.ryerson.ca/~aharley/vis/fc/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources supplémentaires\n",
    "\n",
    "- https://www.math.univ-toulouse.fr/~besse/Wikistat/pdf/st-m-hdstat-rnn-deep-learning.pdf\n",
    "- https://stanford.edu/~shervine/l/fr/teaching/cs-230/pense-bete-reseaux-neurones-convolutionnels\n",
    "\n",
    "Ressources plus générales en deep learning :\n",
    "- https://stanford.edu/~shervine/l/fr/teaching/cs-230/pense-bete-petites-astuces-apprentissage-profond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tpdu] *",
   "language": "python",
   "name": "conda-env-tpdu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
