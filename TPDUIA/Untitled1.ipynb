{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f'Tensorflow version {tf.__version__}')\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-costa",
   "metadata": {},
   "source": [
    "On s'intéresse au data set suivant : `colorectal_histology`, issu de l'article suivant : \n",
    "\n",
    "**Kather, J., Weis, CA., Bianconi, F. et al. Multi-class texture analysis in colorectal cancer histology. Sci Rep 6, 27988 (2016). https://doi.org/10.1038/srep27988**\n",
    "\n",
    "Ce data set consiste en la classification en 8 classes d'images histologiques de taille 150x150x3 de tissus prélevés dans le cadre de l'étude du cancer colorectal.\n",
    "L'image ci-dessous est tirée de l'article de Kather et al., et reprend, pour chaque ligne, l'un des 8 types de tissus à classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-writing",
   "metadata": {},
   "source": [
    "<img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fsrep27988/MediaObjects/41598_2016_Article_BFsrep27988_Fig1_HTML.jpg?as=webp\" width=\"800\">\n",
    "Légende [Figure 1]: \n",
    "\n",
    "_**Representative images from our dataset.** Here, the first 10 images of every tissue class in our dataset are shown. They represent the wide variation of illumination, stain intensity and tissue textures present in routine histopathological images. Images were extracted from 10 independent samples of colorectal cancer (CRC) primary tumours. **(a) tumour epithelium, (b) simple stroma, (c) complex stroma (stroma that contains single tumour cells and/or single immune cells), (d) immune cell conglomerates, (e) debris and mucus, (f) mucosal glands, (g) adipose tissue, (h) background.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-exhibition",
   "metadata": {},
   "source": [
    "### Chargement des données avec `tensorflow_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments communs.\n",
    "args = {\n",
    "    'shuffle_files': True, # on permute aléatoirement les images.\n",
    "    'batch_size': 256, # taille de batch : 256.\n",
    "    'as_supervised': True,\n",
    "    'data_dir': 'datasets',\n",
    "    'with_info': True\n",
    "}\n",
    "# 3 jeux de données : train, validation et test.\n",
    "ds_train, info = tfds.load(\n",
    "    'colorectal_histology',\n",
    "    split='train[:80%]', \n",
    "    **args\n",
    ")\n",
    "ds_val, info = tfds.load(\n",
    "    'colorectal_histology',\n",
    "    split='train[80%:90%]',\n",
    "    **args\n",
    ")\n",
    "ds_test, info = tfds.load(\n",
    "    'colorectal_histology',\n",
    "    split='train[90%:]',\n",
    "    **args\n",
    ")\n",
    "# on affiche les informations associées au data set.\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-saskatchewan",
   "metadata": {},
   "source": [
    "### Type de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-output",
   "metadata": {},
   "source": [
    "### Affichage des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ici on sélectionne le premier batch d'images avec les labels correspondants.\n",
    "def store_data(ds: tf.data.Dataset) -> Tuple[List[np.ndarray], List[int]]:\n",
    "    images, labels = [], []\n",
    "    for batch_img, batch_label in tfds.as_numpy(ds):\n",
    "        for image, label in zip(batch_img, batch_label):\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "images, labels = store_data(ds_train)\n",
    "print(f\"Nombre d'images stockées d'entraînement : {len(images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-frost",
   "metadata": {},
   "source": [
    "On retrouve le même type précédent : `uint8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_0 = images[0]\n",
    "image_0[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-matthew",
   "metadata": {},
   "source": [
    "On reprend la librairie `matplotlib` pour nos images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-audit",
   "metadata": {},
   "source": [
    "Et on affiche 25 images avec les classes correspondantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(images[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Classe {labels[i]}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-composition",
   "metadata": {},
   "source": [
    "On peut en afficher un plus grand nombre, sans les classes, de manière compacte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = axes[i//10, i%10]\n",
    "    ax.imshow(images[i])\n",
    "    ax.axis('off')\n",
    "fig.subplots_adjust(hspace=0, wspace=-0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-insured",
   "metadata": {},
   "source": [
    "## Augmentation des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-violence",
   "metadata": {},
   "source": [
    "Lors de l'apprentissage d'un réseau de neurones profond, on procède généralemet à ce que l'on appelle `data augmentation`.\n",
    "L'augmentation de données a pour but, à chaque itération d'entraînement, de montrer une version différente des images qui ont déjà été utilisées à la précédente itération.\n",
    "Ainsi, le réseau de neurones ne voit que des variations des images d'origine. On essaie par ce biais de favoriser la généralisation de l'algorithme et éviter le phénomène de sur-apprentissage. Cette étape est systématiquement implémentée dans la quasi intégralité des méthodes publiées en deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-batch",
   "metadata": {},
   "source": [
    "## Transformations géométriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-cradle",
   "metadata": {},
   "source": [
    "#### Première étape : re-dimensionner les images à la taille voulue\n",
    "\n",
    "Selon les pplications, on souhaite travailler sur des images de tailles différentes. Une taille plus élevée va impliquer de stocker davantage de résultats intermédiaires, et augmente donc la mémoire occupée par le réseau. A l'inverse, utiliser de petites images limites le stockage des _feature maps_ (résultat d'une convolution entre une image et un noyau de convolution, ou _kernel_), au détriment de la richesse de l'information contenue dans ces images (on va devoir interpoler les pixels, ce qui modifie, comme on l'a vu, l'information initiale contenue dans l'image).\n",
    "\n",
    "#### Deuxième étape : convertir en format _float_\n",
    "\n",
    "Avant de procéder à l'augmentation des données et / ou la normalisation (nous verrons cela après), on convertit les images en format `float` ([0, 1]).\n",
    "Ceci permet de réduire l'intervalle pris par les différentes valeurs d'intensité. Ceci améliore l'entraînement des modèles. En effet, on préférera toujours travailler sur des données dont les valeurs sont proches de 0 plutôt que des valeurs dont le support est [0, 255]. Cette dernière approche peut causer d'importants problèmes de stabilité numérique dans le calcul des gradients !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128 # on passe de 150 à 128.\n",
    "preprocessing_pipeline = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.Resizing(img_size, img_size), # par défaut, on utilise une méthode qui réduit l'aliasing.\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255), # on convertit en floattant.\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-cologne",
   "metadata": {},
   "source": [
    "**Puis on applique toute une série d'augmentation sur les images.**\n",
    "\n",
    "**D'abord, on retourne l'image par effet de miroir.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_augmentation_pipeline = tf.keras.Sequential()\n",
    "geom_augmentation_pipeline.add(tf.keras.layers.Lambda(tf.image.random_flip_up_down))\n",
    "geom_augmentation_pipeline.add(tf.keras.layers.Lambda(tf.image.random_flip_left_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_augment(pipeline: tf.keras.Sequential = None):\n",
    "    if pipeline is None:\n",
    "        pipeline = geom_augmentation_pipeline\n",
    "    fig, axes = plt.subplots(5, 15, figsize=(30, 10))\n",
    "    for i in range(25):\n",
    "        # après reconstruction.\n",
    "        ax = axes[i//5, i%5]\n",
    "        im = images[i]\n",
    "        ax.imshow(im)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Classe {labels[i]}')\n",
    "        # espaces vides.\n",
    "        ax = axes[i//5, i%5+5]\n",
    "        ax.axis('off')\n",
    "        # avant normalisation.\n",
    "        ax = axes[i//5, i%5+10]\n",
    "        im = tf.expand_dims(images[i], 0)\n",
    "        ax.imshow(pipeline(im)[0, ...])\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Classe {labels[i]}')\n",
    "    fig.show()\n",
    "\n",
    "show_augment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-camel",
   "metadata": {},
   "source": [
    "**Puis on peut introduire des rotations de magnitude aléatoire dans un intervalle donné (en degrés).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_augmentation_pipeline.add(\n",
    "     tf.keras.layers.experimental.preprocessing.RandomRotation(factor=(-0.5, 0.5), fill_mode='reflect', interpolation='bilinear')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-albania",
   "metadata": {},
   "source": [
    "**Zoom aléatoire.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_augmentation_pipeline.add(\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=(-0.1, 0.2),\n",
    "            width_factor=None, # on préserve le ratio longueur / largeur.\n",
    "            fill_mode='reflect',\n",
    "            interpolation='bilinear'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-spider",
   "metadata": {},
   "source": [
    "**Modification de la qualité de compression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-coffee",
   "metadata": {},
   "source": [
    "Cette méthode d'augmentation présente un désavantage majeur dans son implémentation actuelle : l'impossibilité de prendre un _batch_ d'images comme paramètres d'entrée.\n",
    "Ceci implique de devoir appliquer la transformation de manière séquentielle sur chacune des images du _batch_, puis de reconstituer le _batch_. En pratique, cela s'avère très coûteux en calcul (et en temps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_augmentation_pipeline.add(\n",
    "    tf.keras.layers.Lambda(\n",
    "        lambda x: tf.image.random_jpeg_quality(x, 60, 100)\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "print(\"Nous n'utilisons pas cette fonction en l'état.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-heath",
   "metadata": {},
   "source": [
    "**Translation sur les axes $x$ et $y$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_augmentation_pipeline.add(\n",
    "    tf.keras.layers.experimental.preprocessing.RandomTranslation(\n",
    "        height_factor=0.1,\n",
    "        width_factor=0.1,\n",
    "        fill_mode='reflect'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-bracelet",
   "metadata": {},
   "source": [
    "## Transformations de teintes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-lending",
   "metadata": {},
   "source": [
    "**Luminance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_augmentation_pipeline = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_augmentation_pipeline.add(\n",
    "    tf.keras.layers.Lambda(\n",
    "        lambda x: tf.image.random_brightness(x, max_delta=0.2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augment(color_augmentation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-vegetation",
   "metadata": {},
   "source": [
    "**Contraste.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_augmentation_pipeline.add(\n",
    "    tf.keras.layers.Lambda(\n",
    "        lambda x: tf.image.random_contrast(\n",
    "            x,\n",
    "            lower=0.5,\n",
    "            upper=1.5\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augment(color_augmentation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-diary",
   "metadata": {},
   "source": [
    "**HUE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_augmentation_pipeline.add(\n",
    "    tf.keras.layers.Lambda(\n",
    "        lambda x: tf.image.random_hue(x, max_delta=0.2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augment(color_augmentation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-anchor",
   "metadata": {},
   "source": [
    "**Saturation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_augmentation_pipeline.add(\n",
    "    tf.keras.layers.Lambda(\n",
    "        lambda x: tf.image.random_saturation(x, lower=0.5, upper=1.5)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augment(color_augmentation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-strike",
   "metadata": {},
   "source": [
    "## Combinaison des augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_pipeline = color_augmentation_pipeline\n",
    "data_augmentation_pipeline.add(geom_augmentation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augment(data_augmentation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-movie",
   "metadata": {},
   "source": [
    "## Normalisation des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-tattoo",
   "metadata": {},
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-weekly",
   "metadata": {},
   "source": [
    "Une pratique courante consiste à normaliser les données avant tout traitement par un réseau de neurones.\n",
    "\n",
    "**Qu'entend-t-on par normalisation ?**\n",
    "\n",
    "#### Première possibilité : \n",
    "\n",
    "- On soustrait, pour chaque canal R, V, B de l'image, la moyenne des intensités R, V et B de **l'ensemble des images du data set d'entraînement** ;\n",
    "- Puis on divise cette différence par l'écart-type calculé sur les mêmes intensités.\n",
    "\n",
    "$$image[..., 0] \\leftarrow \\frac{image[..., 0] - \\mu_0^{global}}{\\sigma_0^{global}} $$\n",
    "\n",
    "avec $\\mu_0^{global}$ et $\\sigma_0^{global}$ étant les moyennes et écart-types calculés sur l'ensemble des valeurs prises par le canal Rouge sur toutes les données d'entraînement.\n",
    "\n",
    "#### Deuxième possibilité : \n",
    "\n",
    "- On soustrait, pour chaque canal R, V, B de l'image, la moyenne des intensités R, V et B de **cette image**;\n",
    "- Puis on divise cette différence par l'écart-type calculé sur les mêmes intensités.\n",
    "\n",
    "$$image[..., 0] \\leftarrow \\frac{image[..., 0] - \\mu_0^{image}}{\\sigma_0^{image}} $$\n",
    "\n",
    "avec $\\mu_0^{image}$ et $\\sigma_0^{image}$ étant les moyennes et écart-types calculés sur l'ensemble des valeurs prises par le canal Rouge sur l'image d'intérêt.\n",
    "\n",
    "#### Laquelle choisir ?\n",
    "\n",
    "Personnellement, j'utilise systématiquement la deuxième méthode pour une raison simple. L'intuition derrière la normalisation est que nous ne voulons pas qu'une image dont, par exemple, les valeurs d'intensité seraient globalement élevées (image avec un fond globalement blanc) activent davantage les neurones du réseau qu'une image sombre (globalement noir). En effet, ce qui nous importe réside certes dans le contraste de l'image, mais cette information est généralement secondaire face à l'ensemble du contenu d'une image. De fait, on ne souhaite pas discriminer trop fortement telle ou telle image selon sa teinte globale. De fait, on préférera que les intensités soient normalisées à l'échelle d'une image, de manière à ce que _toutes_ les images aient des intensités qui se valent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-incidence",
   "metadata": {},
   "source": [
    "**Par exemple, sur notre data set d'entraînement, on isole les 3 composantes des images et on calcule leur moyenne.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(images):\n",
    "    \"\"\"Conversion : `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(images, tf.float32) / 255\n",
    "\n",
    "# on applique la conversion.\n",
    "ds_tmp = ds_train.map(\n",
    "    lambda x, y: (convert(x), y),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "# on stocke de nouveau les images.\n",
    "images, labels = store_data(ds_tmp)\n",
    "print('Données normalisées prêtes !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on stocke les canaux R, V, B.\n",
    "R = np.array([img[..., 0] for img in images])\n",
    "V = np.array([img[..., 1] for img in images])\n",
    "B = np.array([img[..., 2] for img in images])\n",
    "C = [R, V, B]\n",
    "# leur moyenne et écart-type par image.\n",
    "m_R, m_V, m_B = R.mean(axis=(1, 2)), V.mean(axis=(1, 2)), B.mean(axis=(1, 2))\n",
    "s_R, s_V, s_B =  R.std(axis=(1, 2)), V.std(axis=(1, 2)), B.std(axis=(1, 2))\n",
    "gm_R, gm_V, gm_B = R.mean(axis=(0, 1, 2)), V.mean(axis=(0, 1, 2)), B.mean(axis=(0, 1, 2))\n",
    "gs_R, gs_V, gs_B = R.std(axis=(0, 1, 2)), V.std(axis=(0, 1, 2)), B.std(axis=(0, 1, 2))\n",
    "# on affiche les moyennes et écart-types globaux.\n",
    "print(f'Intensité rouge {gm_R} +/- {gs_R}')\n",
    "print(f'Intensité verte {gm_V} +/- {gs_V}')\n",
    "print(f'Intensité bleue {gm_B} +/- {gs_B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-samuel",
   "metadata": {},
   "source": [
    "On les affiche pour quelques images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 15, figsize=(25, 10))\n",
    "cmaps = ['Reds', 'Greens', 'Blues']\n",
    "for i in range(15):\n",
    "    ax = axes[0, i]\n",
    "    ax.imshow(images[i])\n",
    "    ax.axis('off')\n",
    "    for j in range(3):\n",
    "        ax = axes[j+1, i]\n",
    "        ax.imshow(C[j][i], cmap=cmaps[j])\n",
    "        ax.axis('off')\n",
    "fig.subplots_adjust(hspace=0, wspace=0.05)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-blond",
   "metadata": {},
   "source": [
    "Affichons l'histogramme des intensités après normalisation globale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
    "axes[0].hist(((R-gm_R)/gs_R).flatten(), color='r', bins=100)\n",
    "axes[1].hist(((V-gm_V)/gs_V).flatten(), color='g', bins=100)\n",
    "axes[2].hist(((B-gm_B)/gs_B).flatten(), color='blue', bins=100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-cement",
   "metadata": {},
   "source": [
    "De même pour les intensités après normalisation à l'échelle individuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
    "def f(m):\n",
    "    return np.expand_dims(m, axis=(1, 2))\n",
    "\n",
    "axes[0].hist(((R-f(m_R))/f(s_R)).flatten(), color='r', bins=100)\n",
    "axes[1].hist(((V-f(m_V))/f(s_V)).flatten(), color='g', bins=100)\n",
    "axes[2].hist(((B-f(m_B))/f(s_B)).flatten(), color='blue', bins=100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-adjustment",
   "metadata": {},
   "source": [
    "Si l'on s'intéresse désormais à 2 images aux teintes très différentes.\n",
    "D'abord avec la méthode 1 globale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 5))\n",
    "for k, i in enumerate([0, 11]): \n",
    "    axes[k, 0].imshow(images[i])\n",
    "    axes[k, 0].axis('off')\n",
    "    axes[k, 1].hist(((R[i]-gm_R)/gs_R).flatten(), color='r', bins=10)\n",
    "    axes[k, 2].hist(((V[i]-gm_V)/gs_V).flatten(), color='g', bins=10)\n",
    "    axes[k, 3].hist(((B[i]-gm_B)/gs_B).flatten(), color='blue', bins=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-aircraft",
   "metadata": {},
   "source": [
    "Puis avec la méthode 2 individuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 5))\n",
    "cmaps = ['Reds', 'Greens', 'Blues']\n",
    "for k, i in enumerate([0, 11]): \n",
    "    axes[k, 0].imshow(images[i])\n",
    "    axes[k, 0].axis('off')\n",
    "    axes[k, 1].hist(((R[i]-m_R[i])/s_R[i]).flatten(), color='r', bins=10)\n",
    "    axes[k, 2].hist(((V[i]-m_V[i])/s_V[i]).flatten(), color='g', bins=10)\n",
    "    axes[k, 3].hist(((B[i]-m_B[i])/s_B[i]).flatten(), color='blue', bins=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-fantasy",
   "metadata": {},
   "source": [
    "**Pros and cons**\n",
    "- Méthode globale : les valeurs prises par les intensités normalisées ont un support relativement restreint, contrairement à la méthode de normalisation individuelle qui introduit certaines valeurs plus extrêmes à l'occasion de la division par l'écart-type qui, sur des images uniformes, peut être proche de 0 (dans ce cas, les valeurs normalisées explosent). Cependant, entre 2 images de teintes très différentes, les moyennes des distributions des intensités par canaux vont être significativement différente, et introduire un potentiel biais dans l'activation des neurones du réseau si aucun normalisation ultérieure n'est réalisée (_batch-normalisation_).\n",
    "- Méthode individuelle : le support est plus large, mais les pixels associés à la teinte majoritaire voit leur intensité normalisée proche de 0, ce qui rend le réseau plus agnostique à la différence de teinte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-sunset",
   "metadata": {},
   "source": [
    "**Comment visualiser les images normalisées ?** (par exemple avec la méthode 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image: np.ndarray):\n",
    "    im = image.copy()\n",
    "    c0, c1, c2 = im[..., 0], im[..., 1], im[..., 2]\n",
    "    im[..., 0] = (c0 - c0.mean()) / c0.std()\n",
    "    im[..., 1] = (c1 - c1.mean()) / c1.std()\n",
    "    im[..., 2] = (c2 - c2.mean()) / c2.std()\n",
    "    return im\n",
    "    \n",
    "    \n",
    "images_norm = [normalize(img) for img in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-breeding",
   "metadata": {},
   "source": [
    "On s'aperçoit que les images n'ont plus aucun sens en termes de perception visuelle. C'est normal, la normalisation décorrèle complétement les canaux R, V et B entre eux ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    ax = axes[i//5, i%5]\n",
    "    im = images_norm[i]\n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Classe {labels[i]}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-pacific",
   "metadata": {},
   "source": [
    "On peut essayer de retomber sur nos images initiales en faisant la transformation suivante : 1) normaliser dans l'intervalle [0, 1], 2) multiplier par 255, 3) convertir en `uint8`. Cette transformation est un bon proxy de reconstruction de l'image; même si, dès lors que l'on opère une normalisation de ce type, il devient impossible de récupérer l'image initiale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 15, figsize=(30, 10))\n",
    "for i in range(25):\n",
    "    # après reconstruction.\n",
    "    ax = axes[i//5, i%5]\n",
    "    im = images_norm[i]\n",
    "    ax.imshow(((im - im.min(axis=(0, 1)))/(im.max(axis=(0, 1)) - im.min(axis=(0, 1)))*255).astype(np.uint8))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Classe {labels[i]}')\n",
    "    # espaces vides.\n",
    "    ax = axes[i//5, i%5+5]\n",
    "    ax.axis('off')\n",
    "    # avant normalisation.\n",
    "    ax = axes[i//5, i%5+10]\n",
    "    im = images[i]\n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Classe {labels[i]}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-breath",
   "metadata": {},
   "source": [
    "**Normalisation en tensorflow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def convert(images, labels):\n",
    "    Conversion : `uint8` -> `float32`.\n",
    "    return tf.cast(images, tf.float32) / 255., labels\n",
    "\"\"\"\n",
    "# après avoir appliquer la conversion en flottant, on applique la normalisation individuelle sur chaque\n",
    "# canal R, V et B via la function `tf.image.per_image_standardization`.\n",
    "ds_tmp = ds_tmp.map(\n",
    "    lambda x, y: (tf.image.per_image_standardization(x), y),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-hearts",
   "metadata": {},
   "source": [
    "## Conversion, augmentation et normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(\n",
    "    lambda x, y: (preprocessing_pipeline(x), y),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.map(\n",
    "    lambda x, y: (data_augmentation_pipeline(x), y),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-biotechnology",
   "metadata": {},
   "source": [
    "Quelques statistiques sur notre object `ds_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.benchmark(ds_train, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = ds_val.map(\n",
    "    lambda x, y: (preprocessing_pipeline(x), y),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "eff = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False, weights='imagenet', input_tensor=None,\n",
    "    input_shape=None, pooling='avg'\n",
    ")\n",
    "model = tf.keras.Sequential([\n",
    "    eff,\n",
    "    tf.keras.layers.Dense(8, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3))\n",
    "labs, counts = np.unique(labels, return_counts=True)\n",
    "print(f'Labels: {labs} et nombres: {counts}\\n(fréquences {(counts/counts.sum()).round(3)})')\n",
    "arr = ax.bar(labs, counts, align='center')\n",
    "ax.set_xticks(range(8))\n",
    "ax.set_xticklabels([f'classe {i}' for i in range(1, 9)])\n",
    "ax.set_title(\"Nombre d'images par classe.\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=5,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='logs', histogram_freq=0, write_graph=True,\n",
    "            write_images=False, update_freq='batch', profile_batch=(5, 10)\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-bosnia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
